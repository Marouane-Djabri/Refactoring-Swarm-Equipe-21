"""
Orchestrator - G√®re le flux d'ex√©cution des agents
Test-generator -> Auditor -> Fixer -> Judge en boucle
"""

from pathlib import Path
from typing import Dict, List, TypedDict, Annotated, Optional
import operator

from langgraph.graph import StateGraph, END

from src.agents.auditor import AuditorAgent
from src.agents.fixer import FixerAgent
from src.agents.judge import JudgeAgent
from src.agents.test_generator import TestGeneratorAgent
from src.tools.refactoring_tools import RefactoringTools
from src.utils.logger import log_experiment, ActionType


# D√©finir l'√©tat du syst√®me (State Schema)
class RefactoringState(TypedDict):
    """
    √âtat partag√© entre tous les agents dans le graphe LangGraph

    Cet √©tat est pass√© de noeud en noeud et peut √™tre modifi√© par chaque agent.
    """
    # Donn√©es d'entr√©e
    target_dir: str
    python_files: List[Path]

    # Outils partag√©s
    tools: RefactoringTools

    # R√©sultats de l'Auditor
    refactoring_plan: Dict
    audit_completed: bool

    # Indicateur pour TestGenerator
    tests_generated: bool

    # R√©sultats du Fixer
    fix_results: Dict
    fix_completed: bool
    current_iteration: int
    
    # Feedback du Judge
    tests_passed: bool
    test_results: Dict
    error_feedback: Optional[str]
    should_continue: bool
    max_iterations: int

    # R√©sultats du Judge
    test_results: Dict
    tests_passed: bool
    error_feedback: str

    # M√©tadonn√©es
    max_iterations: int
    should_continue: bool
    final_result: Dict


class LangGraphOrchestrator:
    """
    Orchestrateur bas√© sur LangGraph pour g√©rer le workflow multi-agents
    """

    def __init__(self, max_iterations: int = 10, model_name: str = "mistral-large-latest", target_dir: str = "./sandbox"):
        """
        Initialise l'orchestrateur LangGraph
        """
        self.max_iterations = max_iterations
        self.model_name = model_name

        print("=" * 30)
        print("INITIALISATION DU REFACTORING SWARM avec LangGraph")
        print("=" * 30)

        # Initialiser les outils
        print(f"\nüîß Initializing RefactoringTools...")
        self.tools = RefactoringTools(base_sandbox=target_dir)
        self.sandbox_info = self.tools.get_sandbox_info()
        print(f"   ‚úÖ Sandbox: {self.sandbox_info['sandbox_path']}")
        print(
            f"   ‚úÖ Python files in sandbox: {self.sandbox_info['total_python_files']}")
        print(f"   ‚úÖ Test files: {self.sandbox_info['test_files']}")
        print(
            f"   ‚úÖ Backups available: {self.sandbox_info['backups_available']}")

        # Initialiser les 3 agents
        self.auditor = AuditorAgent(model_name=model_name)
        self.fixer = FixerAgent(model_name=model_name)
        self.judge = JudgeAgent(model_name=model_name)
        self.test_generator = TestGeneratorAgent(model_name=model_name)

        # Cr√©er le graphe d'ex√©cution
        self.workflow = self._build_workflow_graph()

        print("\nGraphe LangGraph cr√©√©!")
        print("Noeuds: Auditor -> TestGenerator -> Judge -> Fixer -> (Loop to Judge)")
        print()

    def _build_workflow_graph(self) -> StateGraph:
        """
        Construit le graphe d'ex√©cution avec LangGraph
        """
        # Cr√©er un nouveau graphe d'√©tats
        workflow = StateGraph(RefactoringState)

        # ===== D√âFINIR LES NOEUDS =====

        # Noeud 1: Auditor (analyse)
        workflow.add_node("auditor", self._auditor_node)

        # Noeud 2: Fixer (correction)
        workflow.add_node("fixer", self._fixer_node)

        # Noeud 2.5: TestGenerator (generation de tests unitaires)
        workflow.add_node("test_generator", self._test_generator_node)

        # Noeud 3: Judge (test et validation)
        workflow.add_node("judge", self._judge_node)

        # ===== D√âFINIR LES TRANSITIONS =====

        # START -> Auditor (toujours commencer par l'analyse)
        workflow.set_entry_point("auditor")

        # Auditor -> TestGenerator (plans -> tests)
        workflow.add_edge("auditor", "test_generator")
        
        # TestGenerator -> Judge (tests -> validation initiale)
        workflow.add_edge("test_generator", "judge")

        # Judge -> ? (transition conditionnelle)
        workflow.add_conditional_edges(
            "judge",
            self._should_continue_or_stop,  
            {
                "continue": "fixer",  # Si √©chec -> Fixer
                "stop": END           # Si succ√®s -> Fin
            }
        )

        # Fixer -> Judge (correction -> re-validation)
        workflow.add_edge("fixer", "judge")

        # Compiler le graphe
        app = workflow.compile()

        return app

    # ===== FONCTIONS DES NOEUDS =====

    def _auditor_node(self, state: RefactoringState) -> RefactoringState:
        """
        Noeud Auditor: Analyse tous les fichiers Python
        """
        print("\n" + "=" * 30)
        print("NOEUD: AUDITOR (Analyse)")
        print("=" * 30)

        # Ex√©cuter l'analyse
        refactoring_plan = self.auditor.analyze(Path(state["target_dir"]))

        # Mettre √† jour l'√©tat
        state["refactoring_plan"] = refactoring_plan
        state["audit_completed"] = True
        state["current_iteration"] = 1

        return state

    def _fixer_node(self, state: RefactoringState) -> RefactoringState:
        """
        Noeud Fixer: Corrige le code selon le plan
        """
        print("\n" + "=" * 30)
        print(
            f"NOEUD: FIXER (Correction - Iteration {state['current_iteration']})")
        print("=" * 30)

        refactoring_plan = state["refactoring_plan"]
        iteration = state["current_iteration"]

        # Ex√©cuter la correction
        fix_results = self.fixer.fix_code(
            refactoring_plan, test_errors=state.get("error_feedback"))

        # Mettre √† jour l'√©tat
        state["fix_results"] = fix_results
        state["fix_completed"] = True
        # Note: current_iteration est incr√©ment√© dans _should_continue_or_stop

        return state

    def _test_generator_node(self, state: RefactoringState) -> RefactoringState:
        """
        Noeud TestGenerator: G√©n√®re des tests unitaires pour le code (une seule fois)
        """
        
        # Si les tests ont d√©j√† √©t√© g√©n√©r√©s, on passe (le graphe ne devrait pas repasser par ici, mais s√©curit√©)
        if state.get("tests_generated"):
            print("\n" + "=" * 30)
            print("NOEUD: TEST GENERATOR (Skipped - Already Generated)")
            print("=" * 30)
            return state

        print("\n" + "=" * 30)
        print("NOEUD: TEST GENERATOR (Test Creation)")
        print("=" * 30)
        
        target_dir = state["target_dir"]
        self.test_generator.generate_unit_tests(target_dir)
        
        state["tests_generated"] = True
        return state
        
    def _judge_node(self, state: RefactoringState) -> RefactoringState:
        """
        Noeud Judge: Teste et valide le code corrig√©
        """
        print("\n" + "=" * 30)
        print("NOEUD: JUDGE (Test and Validation)")
        print("=" * 30)

        target_dir = Path(state["target_dir"])

        # Ex√©cuter les tests
        test_results = self.judge.run_tests(target_dir)

        # Mettre √† jour l'√©tat
        state["test_results"] = test_results
        state["tests_passed"] = test_results.get("status") == "success"

        # Si tests √©chouent, pr√©parer le feedback pour le prochain cycle
        if not state["tests_passed"]:
            # Recup√©rer uniquement les tests echou√©s
            failing_tests = test_results.get("failing_tests", [])
            
            # Formater le feedback avec SEULEMENT les tests echou√©s
            feedback_messages = []
            if failing_tests:
                feedback_messages.append("TEST FAILURES (Fix these SPECIFIC errors):")
                for failure in failing_tests:
                    feedback_messages.append(f"""
- File: {failure.get('file', 'unknown')}
- Test: {failure.get('test', 'unknown')}
- Error: {failure.get('error', 'No error message')}
""")
            
            # Si on a des erreurs Pylint (stock√©es dans failing_tests pour le moment par JudgeAgent)
            # Elles seront incluses car _extract_failures les g√®re ou Judge les y met
            
            state["error_feedback"] = "\n".join(feedback_messages)
            
            # Injecter les erreurs dans le plan pour le Fixer
            state["refactoring_plan"]["errors"] = state["error_feedback"]

        return state

    # ===== FONCTION DE D√âCISION =====

    def _should_continue_or_stop(self, state: RefactoringState) -> str:
        """
        D√©cide si on continue le loop ou si on s'arr√™te

        Cette fonction est appel√©e apr√®s le noeud Judge pour d√©terminer
        la prochaine √©tape.
        """
        tests_passed = state["tests_passed"]
        current_iteration = state["current_iteration"]
        max_iterations = state["max_iterations"]

        if tests_passed:
            # Tous les tests passent -> Succ√®s!
            print(f"\\nDECISION: STOP (Tests successful)")
            state["should_continue"] = False
            return "stop"

        elif current_iteration >= max_iterations:
            # Max it√©rations atteint -> √âchec
            print(f"\nDECISION: STOP (Max iterations: {max_iterations})")
            state["should_continue"] = False
            return "stop"

        else:
            # Continuer le loop
            print(
                f"\nDECISION: CONTINUE (Iteration {current_iteration + 1}/{max_iterations})")
            state["current_iteration"] += 1
            state["should_continue"] = True
            return "continue"

    # ===== M√âTHODE PRINCIPALE =====

    def discover_python_files(self, target_dir: Path) -> List[Path]:
        """
        D√©couvre tous les fichiers Python dans le r√©pertoire cible
        """
        python_files = list(target_dir.rglob("*.py"))

        # Filtrer les fichiers de test et __init__.py
        python_files = [
            f for f in python_files
            if not f.name.startswith("test_") and f.name != "__init__.py"
        ]

        print(f"Python files discovered: {len(python_files)}")
        for f in python_files:
            print(f"   ‚Ä¢ {f.relative_to(target_dir)}")

        return python_files

    def validate_sandbox(self, target_dir: str) -> bool:
        """
        Valide que le r√©pertoire cible est accessible et contient des fichiers
        """
        validation = self.tools.validate_target_dir(target_dir)

        if not validation.get("valid"):
            print(f"‚ùå Erreur: {validation.get('error')}")
            return False

        print(f"‚úÖ Sandbox validated: {validation.get('relative_path')}")
        return True

    def run(self, target_dir: str) -> Dict:
        """
        Point d'entr√©e principal: ex√©cute le graphe LangGraph
        """
        target_path = Path(target_dir)

        # Validation du r√©pertoire
        if not target_path.exists():
            error_msg = f"Erreur: The directory '{target_dir}' does not exist!"
            print(error_msg)
            return {"success": False, "error": error_msg}

        print(f"Target directory: {target_path.absolute()}\n")

        # D√©couvrir les fichiers Python
        python_files = self.discover_python_files(target_path)

        if not python_files:
            print("No Python files found initially.")

        # ===== INITIALISER L'√âTAT =====
        initial_state: RefactoringState = {
            "target_dir": str(target_path),
            "python_files": python_files,
            "tools": self.tools,
            "refactoring_plan": {},
            "audit_completed": False,
            "tests_generated": False,
            "fix_results": {},
            "fix_completed": False,
            "current_iteration": 1,
            "test_results": {},
            "tests_passed": False,
            "error_feedback": None,
            "max_iterations": self.max_iterations,
            "should_continue": True,
            "final_result": {}
        }

        print("\n" + "=" * 60)
        print("EXECUTION OF LANGGRAPH")
        print("=" * 60)

        # ===== EX√âCUTER LE GRAPHE =====
        try:
            # Invoquer le graphe avec l'√©tat initial
            final_state = self.workflow.invoke(initial_state)

            # Construire le r√©sultat final
            tests_passed = final_state["tests_passed"]
            iteration = final_state["current_iteration"]
            test_results = final_state["test_results"]

            if tests_passed:
                print("\n" + "=" * 30)
                print("SUCCESS: All tests pass!")
                print("=" * 30)
                print(f"   ‚Ä¢ Iterations needed: {iteration}")

                final_result = {
                    "success": True,
                    "iterations_needed": iteration,
                    "test_result": test_results
                }
            else:
                print("\n" + "=" * 30)
                print(
                    f"FAILURE: Max iterations reached ({self.max_iterations})")
                print("=" * 30)

                final_result = {
                    "success": False,
                    "iterations_needed": self.max_iterations,
                    "reason": "Max iterations reached",
                    "last_test_result": test_results
                }

            # Logger le r√©sultat final
            log_experiment(
                agent_name="LangGraph_Orchestrator",
                model_used=self.model_name,
                action=ActionType.ANALYSIS,
                details={
                    "target_directory": str(target_path),
                    "input_prompt": f"Orchestration LangGraph sur {len(python_files)} fichiers avec RefactoringTools",
                    "output_response": f"Succ√®s: {tests_passed}, It√©rations: {iteration}",
                    "total_files": len(python_files),
                    "max_iterations": self.max_iterations,
                    "final_result": final_result,
                    "graph_execution": "LangGraph workflow completed",
                    "tools_used": {
                        "sandbox_path": self.sandbox_info['sandbox_path'],
                        "backups_created": self.sandbox_info['backups_available'],
                        "test_files": self.sandbox_info['test_files']
                    }
                },
                status="SUCCESS" if tests_passed else "FAILED"
            )

            return final_result

        except Exception as e:
            print(f"\nError executing the graph: {e}")
            import traceback
            traceback.print_exc()

            return {
                "success": False,
                "error": str(e)
            }
